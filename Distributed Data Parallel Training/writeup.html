<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>CS 744 Big Data Systems</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <link href="writeup_files/bootstrap.css" rel="stylesheet">
    <style>
      body {
        padding-top: 20px;
      }
    </style>
    <link href="writeup_files/bootstrap-responsive.html" rel="stylesheet">
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></sc\
ript>
    <![endif]-->
  </head>

  <body>

    <!--div class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <a class="btn btn-navbar" data-toggle="collapse" data-target=".na\
v-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="#">CS 744 Big Data Systems</a>
          <div class="nav-collapse collapse">
            <ul class="nav">
              <li class="active"><a href="/">Home</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div-->

    <div class="container">

      <h1 id="cs744-assignment-2">CS744 Assignment 2</h1>

<h3 id="due-oct-5-2020-10pm-central-time">Due: Oct 5, 2020, 10pm Central Time</h3>

<h2 id="overview">Overview</h2>
<p>This assignment is designed to build on your in-class understanding of how
distributed training of machine learning algorithms is performed. You will get
on hands-on experience in using <a href="https://pytorch.org/">PyTorch</a> in conjunction
with MPI like communication frameworks like
<a href="https://github.com/facebookincubator/gloo">Gloo</a> and
<a href="https://www.open-mpi.org/">OpenMPI</a>. You will understand the tradeoffs of
different approaches of performing distributed training.</p>

<h2 id="learning-outcomes">Learning Outcomes</h2>
<p>After completing this programming assignment, you should be able to:</p>

<ul>
  <li>Deploy and Configure Distributed ML Training frameworks.</li>
  <li>Write distributed training applications with PyTorch</li>
  <li>Understand the trade-off between different methods of performing distributed
training</li>
  <li>Describe how Machine Learning frameworks(PyTorch) interact with collective
communication frameworks (e.g., OpenMPI, Gloo)</li>
</ul>

<h2 id="environment-setup">Environment Setup</h2>
<p>Like previous assignment you will complete this assignment on Cloudlab. See <a href="https://pages.cs.wisc.edu/~shivaram/cs744-fa20/assignment-zero.html">Assignment 0</a>
 to learn how to use CloudLab. You should only create one experiment per
 group and work together.  An experiment lasts 16 hours, which is very 
quick. So, set a time frame that all your group members can sit together
 and focus on the project, or make sure to extend the experiment when it
 is necessary.</p>

<p>In this assignment, we provide you a CloudLab profile called 
“cs744-fa20-assignment2” under “UWMadison744-F20” project for you to 
start your experiment. The profile is a simple 4-node cluster with 
Ubuntu installed on each machine.</p>

<p><strong>While launching the experiment make sure to choose the right group name.</strong>
You get full control of the machines once the experiment is created, so 
feel free to download any missing packages you need in the assignment.</p>

<p>As the first step, you should run the following command on every VM:</p>

<ol>
  <li><code>sudo apt-get update --fix-missing</code></li>
  <li><code>sudo apt install python-pip</code></li>
  <li>Install numpy using: <code>pip install intel-numpy</code></li>
  <li>Install Pytorch for CPU	using
    <pre><code>pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
  </li>
  <li>Install future using: <code>pip install future</code></li>
  <li>Gloo comes built in with PyTorch, therefore requiring no installation.</li>
</ol>

<p>Note 1: For installation using pip may be asked for root permission. Ideally when setting things up you should use
virtual environment but since we have dedicated instances for the project you are free to use <code>sudo pip</code>.
Note 2: If you are familiar feel free to use Anaconda for setup.</p>

<p>For this assignment using the home directory is enough, you will not need to use any extra disk.</p>

<h2 id="part-1-training-vgg-11-on-cifar10">Part 1: Training VGG-11 on Cifar10</h2>
<p>We have provided a base script for you to start with, which provides a
 model setup (model.py) and training setup(main.py) to train on VGG-11 
network with Cifar10 dataset.</p>

<p><strong>You can find the base training scripts to modify <a href="https://pages.cs.wisc.edu/~shivaram/cs744-fa20/assignment2-template.tar.gz">here</a></strong></p>

<p><em>Task1</em>: Fill in the standard training loop of forward pass, 
backward pass, loss computation and optimizer step in main.py. Make sure
 to print the loss value after every 20 iterations. Run training for a 
total of 1 epoch (i.e., until every example has been seen once) with 
batch size 256. (NOTE: If CIFAR-10 has 50,000 examples then you will 
finish 1 epoch after 196 iterations).</p>

<p>There are several examples for training that describe these four steps. Some good resources include the <a href="https://github.com/pytorch/examples">PyTorch examples repository</a> and the <a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html">Pytorch tutorials</a>.
 This script is also a starting point for later parts of the assignment.
 Familiarize yourself with the script and run training for 1 epoch on a 
single machine.</p>

<h2 id="part-2---distributed-data-parallel-training">Part 2 - Distributed Data Parallel Training</h2>
<p>Next you will modify the script used in Part 1, to enable distributed
 data
parallel training. There are primarily two ways distributed training is 
performed i) Data Parallel, ii) Model Parallel. In case of Data parallel
 each of the participating workers train the same network, but on 
different data points from the dataset. After each iteration (forward 
and backward pass) the workers average their local gradients to come up 
with a single update. In model parallel training the model is 
partitioned among number of workers. Each worker performs training on 
part of the model and sends it output to the worker which has the next 
partition during forward pass and vice-versa in backward pass. Model 
parallel is usually used when the size of the network is very large and 
doesn’t fit on a single worker. In this assignment we solely focus on 
Data Parallel Training. For data parallel training you will need to 
partition the data among other nodes. Look at the FAQ’s to find details 
on how to partition the data.</p>

<h3 id="part-2a--sync-gradient-with-gather-and-scatter-call-using-gloo-backend">Part 2a:  Sync gradient with gather and scatter call using Gloo backend</h3>
<p>PyTorch comes with the Gloo backend built-in. We will use this to implement gradient aggregation
using the gather and scatter calls.</p>

<p>(i) Set Pytorch up in distributed mode using the distributed module of PyTorch. For details look
<a href="https://pytorch.org/docs/stable/distributed.html">here</a>. Initialize the distributed environment using <a href="https://pytorch.org/docs/stable/distributed.html">init_process_group</a>.</p>

<p>(ii) Next, to perform gradient aggregation you will need to read the 
gradients after backward pass for each layer. Pytorch performs gradient 
computation using auto grad when you call <code>.backward</code> on a computation graph. The gradient is stored in <code>.grad</code> attribute of the parameters. The parameters can be accessed using <code>model.parameters()</code>.</p>

<p>(iii) Finally, to perform the aggregation you will you use gather and
 scatter communication collectives. Specifically Rank 0 in the group 
will gather the gradients from all the participating workers and perform
 elementwise mean and then scatter the mean vector. The workers update 
the grad variable with the received vector and then continue training.</p>

<p><em>Task2a</em>: Implement gradient sync using gather and scatter. 
Verify that you are using the same total batch size, where total batch 
size = batch size on one machine * num_of machines. With the same total 
batch size you should get similar loss values as in the single node 
training case. Remember you trained with total batch size of 256 in Task
 1.</p>

<h3 id="part-2bsync-gradient-with-allreduce-using-gloo-backend">Part 2b:Sync gradient with allreduce using Gloo backend</h3>
<p>Ring Reduce is an extremely scalable technique for performing gradient synchronization. Read <a href="https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/">here</a> about how ring reduce has been applied to distributed machine learning.
Instead of using the scatter and gather collectives separately, you will next use the built in <code>allreduce</code>
 collective to sync gradients among different nodes. Again read the 
gradients after backward pass layer by layer and perform allreduce on 
the gradient of each layer. Note the PyTorch allreduce call doesn’t have
 an ‘average’ mode. You can use the ‘sum’ operation and then get the 
average on each node by dividing with number of workers participating. 
After averaging, update the gradients of the model as in the previous 
part.</p>

<p><em>Task2b</em>: Implement gradient sync using allreduce collective in Gloo. In this
case if you have set the same random seed (see FAQ), you should see the same loss value as in Task2a,
while using the same total batch size of 256.</p>

<h2 id="part-3-distributed-data-parallel-training-using-built-in-module">Part 3: Distributed Data Parallel Training using Built in Module</h2>
<p>Now instead of writing your own gradient synchronization, use the distributed functionality
provided by PyTorch. Register your model with <a href="https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">distributed data
parallel</a>
and perform distributed training. Unlike in Part 1 and Part 2 you will not need
to read the gradients for each layer as DistributedDataParallel performs these steps
automatically. For more details read <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">here</a>.</p>

<h2 id="deliverables">Deliverables</h2>
<p>You should submit a tar.gz file to Canvas, which consists of a brief report
(filename: groupx.pdf) and the code of each task. In the report include the
following contents:</p>
<ul>
  <li>Discard the timings of first
iteration and report <em>avg time per iteration</em> for the remaining 9 iterations for each task (1, 2a, 2b, 3).</li>
  <li>In the context of the <a href="https://arxiv.org/pdf/2006.15704.pdf">PyTorch distributed paper</a>, reason about the difference or lack of difference among different setups.</li>
  <li>Comment on the scalability of distributed machine learning based on your results.</li>
  <li>Add contributions of each member of the group.</li>
  <li>Code for each part should be in different folders.</li>
  <li>All your codes should be runnable using the following command-
    <pre><code>python main.py --master-ip $ip_address$ --num-nodes 4 --rank $rank$
</code></pre>
    <p>Look at python argparse to provide this functionality. 
where IP-address, num-nodes and rank are command line parameters supplied by the grader at run time.</p>
  </li>
  <li>Provide any other implementation details</li>
</ul>

<h2 id="faqs">FAQ’s</h2>
<p><strong>Testing Programs</strong> We suggest you to write small programs to test the
functionality of  communication collectives at first. For example, create a tensor and send it to
another nodes. Next try to perform all reduce on it. This will help you get
comfortable with communication collectives.</p>

<p><strong>Using Experiment Network</strong> Same as previous assignments please use the
experimental network for this assignment. This means you need to make sure the machines listen on
10.10.1.* interfaces.</p>

<p><strong>Example of distributed setup</strong> Look at PyTorch <a href="https://github.com/pytorch/examples/blob/master/imagenet/main.py">Imagenet</a> and <a href="https://github.com/pytorch/examples/tree/master/distributed/ddp">distributed</a> examples.</p>

<p><strong>Setting up distributed init</strong> In this setup we will 
use init-method as “tcp://10.10.1.1:6585”. Instead of a shared file 
system we want to use TCP to communicate. In this example I am using 
10.10.1.1 is the IP-address for rank 0. Port has to be a non-privileged 
port, i.e. greater than 1023.</p>

<p><strong>Running the program</strong> If you are using an MPI setup you can use <code>mpi run</code> command to launch
multiple workers. Since we have a small number of nodes we will manually start
our program on the 4 nodes and keep all arguments same except the rank. That is, ssh into each node
and run <code>python main.py --rank &lt;fill-in-appropriate-rank&gt;</code></p>

<p><strong>Rank and World Size</strong> Rank is indexed from 0, World size is number of worker
nodes. So in our case Rank will be from 0 to 3 and World Size will be 4</p>

<p><strong>Setting up random seed</strong> You will need to setup the 
random seed before you initialize the model, since the model is 
initialized randomly. In Data Parallel setting you need to make sure you
 start from the same model on all the workers.
Look at <code>torch.manual_seed()</code> and <code>numpy.random.seed()</code>.</p>

<p><strong>Data Partitioning</strong> In case of data parallel training, the workers
train on non-overlapping data partitions. You will use the distributed sampler
to distribute the data among workers. For more details look at
<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler">torch.utils.data.distributed_sampler</a></p>


    </div>

  

</body></html>